# Mathematical Foundations for ML and AI

## Linear Algebra
### Vector and Matrix Norms
**Norm**: function used to measure the magnitude of a 
- Norms map vectors to non-negative values
- The norm vector **x** measures the distance from the origin to the point **x**

**Norm Types**
- L<sub>p</sub><br>
<img src="https://latex.codecogs.com/gif.latex?\left&space;\|&space;x&space;\right&space;\|_{p}&space;=&space;\left&space;(&space;\sum_{i}&space;\left&space;|&space;x_{i}&space;\right&space;|^{p}\right&space;)^{\frac{1}{p}}" title="Lp Norm" />

- Eucledean Norm (L<sup>2</sup> norm or ||**x**||): most commonly used norm function.<br>
<img src="https://latex.codecogs.com/gif.latex?\left&space;\|&space;x&space;\right&space;\|_{2}&space;=&space;\left&space;(&space;\sum_{i}&space;\left&space;|&space;x_{i}&space;\right&space;|^{2}\right&space;)^{\frac{1}{2}}" title="Eucledean Norm" />

- L<sup>1</sup>: used in cases where discriminating between small, nonzero values and zero is important. Increases linearly as elements of **x** increase.<br>
<img src="https://latex.codecogs.com/gif.latex?\left&space;\|&space;x&space;\right&space;\|_{1}&space;=&space;\sum_{i}&space;\left&space;|&space;x_{i}&space;\right&space;|" title="L1 Norm" />

- Max Norm (L<sup>inf</sup>): simplifies to the absolute value of the largest element in the vector. <br>
<img src="https://latex.codecogs.com/gif.latex?\left&space;\|&space;x&space;\right&space;\|_\infty&space;=&space;max_{i}&space;\left&space;|&space;x_{i}&space;\right&space;|" title="Max Norm" />

- Frobenius Norm: Analogous to L<sup>2</sup> norm for a vector used in matrices.<br>
<img src="https://latex.codecogs.com/gif.latex?\left&space;\|&space;A&space;\right&space;\|_F&space;=&space;\sqrt{&space;\sum_{i,j}&space;A_{i,j}^2&space;}" title="Frobenius Norm" />

### Vectors, Matrices, and Tensors in Python
See vector_matrices_tensors_in_python.ipynb file

### Special Matrices and Vectors
#### Diagonal Matrices
A matrix is diagonal if the following condition is met:<br>
<img src="https://latex.codecogs.com/gif.latex?D_{i,j}&space;=&space;0&space;for&space;all&space;i&space;\neq&space;j" title="Diagonal Matrices" /></a>

Example of a diagonal matrix: <br>
<img src="https://latex.codecogs.com/gif.latex?\begin{bmatrix}&space;1&space;&&space;0&space;&&space;0\\&space;0&space;&&space;2&space;&&space;0\\&space;0&space;&&space;0&space;&&space;3&space;\end{bmatrix}" title="Diagonal Matrix" />

- Multiplying by a diagonal matrix is computational efficient. For *diag(**v**)**x***, we only need to scale each element *x<sub>i</sub>* by *v<sub>i</sub>*<br><img src="https://latex.codecogs.com/gif.latex?\begin{bmatrix}&space;1&space;&&space;0&space;&&space;0\\&space;0&space;&&space;2&space;&&space;0\\&space;0&space;&&space;0&space;&&space;3&space;\end{bmatrix}&space;*&space;\begin{bmatrix}&space;1&space;&&space;1&space;&&space;1\\&space;1&space;&&space;1&space;&&space;1\\&space;1&space;&&space;1&space;&&space;1&space;\end{bmatrix}&space;=&space;\begin{bmatrix}&space;1(1)&space;&&space;1(1)&space;&&space;1(1)\\&space;2(1)&space;&&space;2(1)&space;&&space;2(1)\\&space;3(1)&space;&&space;3(1)&space;&&space;3(1)&space;\end{bmatrix}" title="Diagonal Matrix Multiplication" />
- In some cases, computationally efficient algorithms are designed by restricting some matrices to be diagonal
- Diagonal matrices need not be square

#### Symmetric Matrices
Symmetric matrices are matrices equal to their transpose<br>
**A** = **A**<sup>T</sup>

For example:<br>
<img src="https://latex.codecogs.com/gif.latex?\begin{bmatrix}&space;1&space;&&space;2&space;&&space;3\\&space;2&space;&&space;3&space;&&space;4\\&space;3&space;&&space;4&space;&&space;5&space;\end{bmatrix}" title="Symmetric Matrix" />

- Symmetric matrices often arise when the entries are generated by some function of two arguments that does not depend on the order of arguments. Eucledian distance is symmetric, so distance matrices will often be symmetric matrices.

#### Unit Vector
A unit vector is a vector with unit norm: <br>
<img src="https://latex.codecogs.com/gif.latex?\left&space;\|&space;x&space;\right&space;\|_{2}&space;=&space;1" title="Unit Vector" />

A unit vector can be obtained by normalizing any vector.

##### Vector Normalization
Normalization is the process of diving a vector by its magnitude, which produces a unit vector.<br>
<img src="https://latex.codecogs.com/gif.latex?\frac{x}{\left&space;\|&space;x&space;\right&space;\|_{2}}&space;=&space;unit\_vector" title="Vector Normalization" />

#### Orthogonality
A vector **x** and a vector **y** are **orthogonal** to each other if **x** \* **y** = 0

If two vectors are orthogonal and have a nonzero magnitude in a 2D space, they will be at a 90 degree angle to each other.

If two vector are orthogonal and unit vectors, they are **orthonormal**.

Orthogonal matrices are matrices whose rows and columns are mutually orthonormal. This is not commonly used in artificial intelligence.

### Eigenvalues and Eigenvectors
#### Eigendecomposition
Eigendecomposition is breaking mathematical objects into their constituent parts. For example: breaking up integer 10 into its prime factors: 2 and 5. We can conclude that 10 is not divisible by 3 and any product of 10 will also be divisible by 5 and 2.

We can also apply this to matrices to reveal functional properties that are not obvious from the representation of the matrix as an array of elements.

#### Eigenvectors and Eigenvalues
An eigenvector of a square matrix **A** is a nonzero vector **v** such that multiplication by **A** alters only the scale of **v**.<br>
<img src="https://latex.codecogs.com/gif.latex?Av&space;=&space;\lambda&space;v" title="Av = \lambda v" /><br>
where:
- **v** = eigenvector
- <img src="https://latex.codecogs.com/gif.latex?\lambda" title="\lambda" /> = a scalar, the eigenvalue corresponding to **v**

#### Eigendecomposition
If a matrix **A** has *n* linearly independent eigenvectors we ca nform a matrix **V** with one eigenvector per columns, and a vector <img src="https://latex.codecogs.com/gif.latex?\lambda" title="\lambda" /> of all the eigenvalues.

The eigendecomposition of **A** is given by:<br>
<img src="https://latex.codecogs.com/gif.latex?A&space;=&space;V&space;diag(\lambda)V^{-1}" title="Eigendecomposition" />

#### Inverse Matrix
<img src="https://latex.codecogs.com/gif.latex?AA^{-1}=A^{-1}A=I" title="Inverse Matrix" /><br>
where **I** is the identity matrix.

The identity matrix is a square matrix with 1 on the diagonal and 0 elsewhere.<br>
<img src="https://latex.codecogs.com/gif.latex?I&space;=&space;\begin{bmatrix}&space;1&space;&&space;0&space;&&space;0\\&space;0&space;&&space;1&space;&&space;0\\&space;0&space;&&space;0&space;&&space;1&space;\end{bmatrix}" title="Identity Matrix" />

#### Eigendecomposition Properties
- Not every matrix can be decomposed into eigenvalues and eigenvectors.
- A matrix is singular (the inverse does not exist) if any of the eigenvalues are zero.
- A matrix with all positive eigenvalues is called a positive definite.
- A matrix with all positive or zero eigenvalues is positive semidefinite.
- A matrix with all negative eigenvalues is negative definite.
- A matrix with all negative or zero eigenvalues is negative semidefinite.

#### Application of Eigendecomposition
Eigenvalue decomposition is mainly used in principal component analysis (PCA).

PCA is a statistical procedured used to convert a set of observations of possibly correlated variables into a set of values of lineraly uncorrelated variables called principal components.

PCA is used for summarizing or compressing the data.

## Multivariate Calculus
### Introduction to Derivatives
**Definition**<br>
- From a geometric perspective it is the slope of a curve. How much does the line change at that point.
- From a physical concept it is the rate of change

#### Derivative basics
The derivative operator:<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}" title="\frac{d}{dx}" /><br>
where *d* implies derivative and the *x* indicates which variable will the derivative be taken of.

Different notations:<br>
If <img src="https://latex.codecogs.com/gif.latex?y=f(x)" title="Different notations" /> then <img src="https://latex.codecogs.com/gif.latex?\frac{dy}{dx}&space;=&space;\frac{df(x)}{dx}&space;=&space;\frac{d}{dx}f(x)=f'(x)" title="Different notations" />

#### Derivative Rules
- Constant <br>
<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}c=0" title="Derivative Rules" />; for example
<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}62=0" title="Derivative Rules Example" />

- Power rule<br>
<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}x^{n}=nx^{n-1}" title="Derivative Rules" />; for example
<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}x^{3}=3x^{2}" title="Derivative Rules Example" />

- Multiplication<br>
<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}cx^{n}=ncx^{n-1}" title="Derivative Rules" />; for example
<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}4x^{3}=4(3)x^{3-1}=12x^2" title="Derivative Rules Example" />

- Sum rule<br>
<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}(f(x)+g(x))=\frac{d}{dx}f(x)+\frac{d}{dx}g(x)" title="Derivative Rules" />; for example
<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}(4x+2x^{2})=4\frac{d}{dx}x+2\frac{d}{dx}x^{2}=4+4x" title="Derivative Rules Example" />

- Product rule <br>
<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}(f(x)g(x))=g(x)\frac{d}{dx}f(x)+f(x)\frac{d}{dx}g(x)" title="Derivative Rules"/>; for example
<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}(x^{2}x)=x\frac{d}{dx}x^{2}+x^{2}\frac{d}{dx}x=x(2x)+x^{2}(1)=2x^{2}+x^{2}=3x^2" title="Derivative Rules Example" />

- Chain rule <br>
<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}(f(g(x)))=f'(g(x))g'(x)" title="Derivative Rules"/>; for example
<img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}sin(x^{2})=\frac{d}{dx}sin(x^{2})\frac{d}{dx}x^{2}=cos(x^{2})(2x)=2xcos(x^{2})">

#### Partial Derivatives
Used for functions with more than one variable.

Example:

<img src="https://latex.codecogs.com/gif.latex?f(x,y)=3x^{2}y" />

<table>
    <tr>
        <td>1. <img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}" /> and treat <i>y</i> as a constant</td>
        <td>2. <img src="https://latex.codecogs.com/gif.latex?\frac{d}{dy}" /> and treat <i>x</i> as a constant</td>
    </tr>
    <tr>
        <td>
            <img src="https://latex.codecogs.com/gif.latex?\frac{d}{dx}f(x,y)" /><br>
            <img src="https://latex.codecogs.com/gif.latex?=\frac{d}{dx}3x^{2}y" /><br>
            <img src="https://latex.codecogs.com/gif.latex?=3y\frac{d}{dx}x^{2}" /><br>
            <img src="https://latex.codecogs.com/gif.latex?=3y(2x)" /><br>
            <img src="https://latex.codecogs.com/gif.latex?=6xy" />
        </td>
        <td>
            <img src="https://latex.codecogs.com/gif.latex?\frac{d}{dy}f(x,y)" /><br>
            <img src="https://latex.codecogs.com/gif.latex?=\frac{d}{dy}3x^{2}y" /><br>
            <img src="https://latex.codecogs.com/gif.latex?=3x^{2}\frac{d}{dx}y" /><br>
            <img src="https://latex.codecogs.com/gif.latex?=3x^{2}" />
        </td>
    </tr>
</table>

These are the partial derivatives with respect to other variable.

### Introduction to Integration
Integration is used to find the area under the curve. It is the reverse process of differentiation.

<img src="https://latex.codecogs.com/gif.latex?\int&space;f'(x)dx=f(x)+C" />

Where *C* is the constant of integration.

<table>
<caption>Types of Derivatives</caption>
    <tr>
        <th>Indefinite</th>
        <th>Definite</th>
    </tr>
    <tr>
        <td>
            Integrals with no limits<br>
            <img src="https://latex.codecogs.com/gif.latex?\int&space;f(x)dx" />
        </td>
        <td>
            Integrals with limits<br>
            <img src="https://latex.codecogs.com/gif.latex?\int^{a}_{b}&space;f(x)dx" />
        </td>
    </tr>
</table>

#### Integration Rules
- Power Rule<br>
<img src="https://latex.codecogs.com/gif.latex?\int&space;x^{n}dx=\frac{x^{n+1}}{n+1}+C" />; for example
<img src="https://latex.codecogs.com/gif.latex?\int&space;x^{2}dx=\frac{x^{3}}{3}+C" />

- Constants<br>
<img src="https://latex.codecogs.com/gif.latex?\int&space;kdx=kx+C" />; for example
<img src="https://latex.codecogs.com/gif.latex?\int&space;4dx=4x+C" />

#### Evaluating Definite Integrals
<img src="https://latex.codecogs.com/gif.latex?\int^{a}_{b}&space;f(x)dx=f(x)+C" /><br>
(value of <img src="https://latex.codecogs.com/gif.latex?f(x)+C" /> at *x* = *b*)-(value of <img src="https://latex.codecogs.com/gif.latex?f(x)+C" /> at *x*=*a*)<br>
<img src="https://latex.codecogs.com/gif.latex?\int^{a}_{b}&space;f'(x)dx=f(b)-f(a)" /><br>
<img src="https://latex.codecogs.com/gif.latex?\int^{a}_{b}&space;f'(x)dx=f(x)|^{a}_{b}=f(b)-f(a)" />

For example:

<img src="https://latex.codecogs.com/gif.latex?\int^{2}_{0}&space;3x^{2}dx" /><br>
<img src="https://latex.codecogs.com/gif.latex?=3\int^{2}_{0}&space;x^{2}dx" /><br>
<img src="https://latex.codecogs.com/gif.latex?=3\frac{x^{3}}{3}|^{2}_{0}" /><br>
<img src="https://latex.codecogs.com/gif.latex?=x^{3}|^{2}_{0}" /><br>
<img src="https://latex.codecogs.com/gif.latex?=2^{3}-0^{3}" /><br>
So the area under the curve between *x*=0 and *x*=2 is 8.

### Vector Calculus: Gradients
Partial derivatives can be organised as vectors.

<img src="https://latex.codecogs.com/gif.latex?\Delta&space;=&space;\begin{bmatrix}&space;\frac{d}{dx}&space;&&space;\frac{d}{dy}&space;\end{bmatrix}" />

<img src="https://latex.codecogs.com/gif.latex?\Delta&space;f(x,y)&space;=&space;\begin{bmatrix}&space;\frac{df(x,y)}{dx}&space;&&space;\frac{df(x,y)}{dy}&space;\end{bmatrix}" />

For example:

<img src="https://latex.codecogs.com/gif.latex?f(x,y)=3x^{2}y" /><br>
<img src="https://latex.codecogs.com/gif.latex?\Delta&space;3x^{2}y&space;=&space;\begin{bmatrix}&space;\frac{d}{dx}3x^{2}y&space;&&space;\frac{d}{dy}3x^{2}y&space;\end{bmatrix}=\begin{bmatrix}&space;6xy&space;&&space;3x^{2}&space;\end{bmatrix}" />

The gradient represents the rate of change. It points in the direction of the greates rate of increase of the function, and its magnitude is the slope in that direction.

Gradient vectors can be in *n* dimensions.

#### Jacobian Matrix
Matrix to organise multiple functions of partial derivatives

<img src="https://latex.codecogs.com/gif.latex?J=\begin{bmatrix}&space;\Delta&space;f(x,y)\\&space;\Delta&space;g(x,y)&space;\end{bmatrix}&space;=&space;\begin{bmatrix}&space;\frac{df(x,y)}{dx}&space;&&space;\frac{df(x,y)}{dy}\\&space;\frac{dg(x,y)}{dx}&space;&&space;\frac{dg(x,y)}{dy}&space;\end{bmatrix}" />

**The general case**

<img src="https://latex.codecogs.com/gif.latex?f(x,y,z)\rightarrow&space;f(x)" /> where 
<img src="https://latex.codecogs.com/gif.latex?x=\begin{bmatrix}&space;x_1\\&space;x_2\\&space;...\\&space;x_n&space;\end{bmatrix}" />

<img src="https://latex.codecogs.com/gif.latex?J=\begin{bmatrix}&space;\Delta&space;f_1(x)\\&space;\Delta&space;f_2(x)\\&space;...\\&space;\Delta&space;f_m(x)\\&space;\end{bmatrix}&space;=&space;\begin{bmatrix}&space;\frac{d}{dx_1}f_1(x)&space;&&space;\frac{d}{dx_2}f_1(x)&space;&&space;...&space;&&space;\frac{d}{dx_n}f_1(x)\\&space;\frac{d}{dx_1}f_2(x)&space;&&space;\frac{d}{dx_2}f_2(x)&space;&&space;...&space;&&space;\frac{d}{dx_n}f_2(x)\\&space;...&space;&&space;...&space;&&space;...&space;&&space;...&space;\\&space;\frac{d}{dx_1}f_m(x)&space;&&space;\frac{d}{dx_2}f_m(x)&space;&&space;...&space;&&space;\frac{d}{dx_n}f_m(x)\\&space;\end{bmatrix}" />

### Mathematical Optimisation
The optimisation problem:

Minimize *f*<sub>0<sub>(*x*) --> also known as the objective function where *x* is the optimisation variable. These are the parameters we want to update.

So that:

<img src="https://latex.codecogs.com/gif.latex?f_{i}(x)\leq&space;0,&space;i=\{1,...,k\}" /><br>
<img src="https://latex.codecogs.com/gif.latex?h_{j}(x)=0,j=\{1,...,l\}" />

These are the constraint functions.

**Difference between local and global critical points**<br>
Critical points: maximum or minimum. Defined by the derivative of the slope = 0. 

There are global and local critical points. Global critical points are the absolute largest/lowest points in the graph. Local critical points are the largest/lowest points in their immediate area.

#### Convex Optimisation
If the objective function is convex, the optimisation problem is called convex optimisation. A convex function is such that when drawing a line between any two points of the function curve, all points in the curve are lower than any point in the line other than the origin and final points of the line.

<img src="convex_function.png" />

#### Finding Extrema (Critical Points) with Calculus
Calculus can be used to find extrema for an objective function. For example:

<img src="https://latex.codecogs.com/gif.latex?y=x^{2}+3" />

<img src="sample_function.png" />

The derivative at the critical point will be = 0.

1. Find the first derivative of the *f(x)*<br>
y=x<sup>2</sup>+3<br>
y'=2x

2. Set the derivative = 0
0=2x
x=0 --> this is a critical point

3. Do the second derivative test
y'=2x<br>
y''=2 --> if the second derivative is positive we have a minimum, if negative we have a maximum.

If we can prove that the function is a convex function, there is no need to do the second derivative test as we would be guaranteed to be at the global critical point.

In machine learning the optimisation process is iterative by starting with a guess and then iterating.

## Probability Theory
### Intro to Probability
- Sources of uncertainty
  - Inherent stochasticity in the system being modeled
  - Incoplete observability
  - Incomplete modelling
- Types of probability
  - Frequentist: the frequency of events.
  - Bayesian probability: a degree of belief
Both types of proabilities follow the same anxioms and are modelled the same.

### Probability Distributions
Random variables
- a variable that can take different values randomly
- a probability distribution specifies how likely each value is to occur
- may be discrete (finite or countable number of states) or continuous

#### Probability Distributio for Distcrete Variables
**Probability Mass Function**: Probability distribution for discrete random variables 
- Criteria for PMF:
  - the domain of *P* must be the set of all possible states of *x*
  - <img src="https://latex.codecogs.com/gif.latex?0\leq&space;P(x)\leq&space;1" />
  - <img src="https://latex.codecogs.com/gif.latex?\sum&space;P(x)=1" /> --> the sumation over all possible states of *P(x)* = 1. This is known as being normalised.
  
**Joint Probability Distributions**: PMF that acts on multiple variables<br>
*P(x=x,y=y)* denotes the probability that *x=x* and *y=y* simultaneously.

**Uniform Distribution**: probability distribution where each state of the distribution is equally likely.This distribution is normalised.<br>
<img src="https://latex.codecogs.com/gif.latex?P(x=x_{i})=\frac{1}{k}" /><br>
<img src="https://latex.codecogs.com/gif.latex?\sum_{i}&space;P(x=x_{i})=\sum_{i}&space;\frac{1}{k}=\frac{k}{k}=1" />

#### Probability Distribution for Continuous Variables
**Probability Density Function**: probability distributions for continuous variables.

To be a PDF, the function *p* must satisfy:
- The domain of *p* must be the set of all positive states of *x*

- <img src="https://latex.codecogs.com/gif.latex?p(x)\geq&space;0" />

- <img src="https://latex.codecogs.com/gif.latex?\int&space;p(x)dx=1" />

#### Marginal Probability
Probability distribution over a subset of all the variables.

**With discrete random variables**<br>
If we know *P(x,y)*, we can find *P(x)* with the sum rule:<br>
<img src="https://latex.codecogs.com/gif.latex?P(\texttt{x}=x)=\sum_{y}P(\texttt{x}=x,\texttt{y}=y)" />

**With continuous random variables**<br>
<img src="https://latex.codecogs.com/gif.latex?P(x)=\int&space;p(x,y)dy" />

#### Conditional Probability
The probability of some event given that some other event has happened.<br>
<img src="https://latex.codecogs.com/gif.latex?P(\texttt{y}=y|\texttt{x}=x)=\frac{P(\texttt{y}=y,\texttt{x}=x)}{P(\texttt{x}=x)}" />

### Expectation, Variance, and Covariance
- Expectation: The expectation of some function *f(x)* with respect to some probability distribution *P(x)* is the mean value *f* takes on when *x* is drawn from *P*.
  - For discrete variables <img src="https://latex.codecogs.com/gif.latex?\texttt{E}_{x\sim&space;P}[f(x)]=\sum_x&space;P(x)f(x)" />
  - For continuous variables <img src="https://latex.codecogs.com/gif.latex?\texttt{E}_{x\sim&space;p}[f(x)]=\int&space;p(x)f(x)dx" />
  
- Variance (<img src="https://latex.codecogs.com/gif.latex?\sigma^{2}" />): the expectation of the squared deviation of a random variable from its mean. It measures how far random numbers drawn from a probability distribution *P(x)* are spread out from their average value.<br>
<img src="https://latex.codecogs.com/gif.latex?\texttt{Var}(f(x))=\texttt{E}[(f(x)-\texttt{E}[f(x)])^{2}]" />

- Standard Deviation(<img src="https://latex.codecogs.com/gif.latex?\sigma" />): <img src="https://latex.codecogs.com/gif.latex?\sqrt{\sigma^{2}}" />

- Covariance: a measure of how much two variables are related to each other. 
<img src="https://latex.codecogs.com/gif.latex?\texttt{Cov}(f(x),g(y))=\texttt{E}[(f(x)-\texttt{E}[f(x)])(g(y)-\texttt{E}[g(y)])]" />
  - High absolute value - values are both far from their respective means at the same time.
  - Positive: both variables take on large values simultaneously.
  - Negative: variables take inversely large values.
  - Covariance is affected by scale
  - Covariance and dependence are related but distinct concepts. Two independent variables have zero covariance but it is possible for dependent variables to have zero covariance if their relationship is non-linear. Independence already excludes non-linear relationships.
  - Commonly used in AI to compress input data or produce better output results when input variables are identified as linearly dependent.

- Covariance Matrix: the covariance matrix of a random vector **x** is an *n* x *n* matrix such that:<br>
<img src="https://latex.codecogs.com/gif.latex?\texttt{Cov}(\textbf{x})_{i,j}=\texttt{Cov}(x_{i},x_{j})" />
  - The diagonal elements of the covariance matrix give the variance: <img src="https://latex.codecogs.com/gif.latex?\texttt{Cov}(x_{i}, x_{i})=\texttt{Var}(x_{i})" />